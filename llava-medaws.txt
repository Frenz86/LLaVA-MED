###p3.2xlarge -- 100gb disco fisso 30gb ram

sudo apt update
sudo apt upgrade -y
sudo apt install -y unzip
sudo apt install -y python3-pip
sudo apt install -y nvidia-utils-535
sudo apt install -y nvidia-driver-535
sudo add-apt-repository ppa:flexiondotorg/nvtop
sudo apt install -y nvtop
sudo reboot

nvidia-smi
nvtop

git clone https://github.com/atultiwari/LLaVA-Med

wget https://hanoverprod.z21.web.core.windows.net/med_llava/models/data_RAD-9epoch_delta.zip
unzip data_RAD-9epoch_delta.zip -d delta_RAD9

pip install --upgrade pip 
pip uninstall torch torchvision -y 
pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117 
pip install openai==0.27.8 
pip uninstall transformers -y 
pip install git+https://github.com/huggingface/transformers@cae78c46 
pip install -e . 
pip install einops ninja open-clip-torch 
pip install flash-attn --no-build-isolation 
pip install accelerate 
pip install fastapi -q
pip install uvicorn -q
pip install httpx==0.24.1
pip install gradio==3.23.0
pip install markdown2[all]
pip install sentencepiece
pip install tokenizers==0.12.1
pip install gradio_client==0.2.7


sudo apt-get -y install -qq aria2

aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/resolve/main/model-00001-of-00002.safetensors -d hf_llama_7b -o model-00001-of-00002.safetensors
aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/resolve/main/model-00002-of-00002.safetensors -d hf_llama_7b -o model-00002-of-00002.safetensors
aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/model.safetensors.index.json -d hf_llama_7b -o model.safetensors.index.json
aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/special_tokens_map.json -d hf_llama_7b -o special_tokens_map.json
aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/resolve/main/tokenizer.model -d hf_llama_7b -o tokenizer.model
aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/tokenizer_config.json -d hf_llama_7b -o tokenizer_config.json
aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/config.json -d hf_llama_7b -o config.json
aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/Llama-2-7b-chat-hf/raw/main/generation_config.json -d hf_llama_7b -o generation_config.json

cd LLaVA-Med

python3 -m llava.model.apply_delta \
    --base /home/ubuntu/hf_llama_7b \
    --target /home/ubuntu/LLaVA-Med-7B \
    --delta /home/ubuntu/delta_RAD9/data_RAD-9epoch_delta

### terminal1
cd LLaVA-Med 
python3 -m llava.serve.controller --host 0.0.0.0 --port 10053

### terminal2
cd LLaVA-Med 
python3 -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10053 --port 40000 --worker http://localhost:40000 --model-path /home/ubuntu/LLaVa-Med-7B --multi-modal


### terminal3 gradio
cd LLaVA-Med
python3 -m llava.serve.gradio_web_server --controller http://localhost:10053

#### open port 7860 for GRADIO on aws






